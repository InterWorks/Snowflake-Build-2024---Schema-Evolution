{
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "id": "3775908f-ca36-4846-8f38-5adca39217f2",
   "metadata": {
    "language": "python",
    "name": "py__imports",
    "collapsed": false,
    "codeCollapsed": false
   },
   "source": "# Import python packages\nimport streamlit as st\nimport gzip\nimport json\nimport pyarrow.parquet as pq\n\n# Import Snowpark packages\nfrom snowflake.snowpark.context import get_active_session\nsnowflake_session = get_active_session()",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "46e10807-c07e-4e97-8eb1-64ac59409bb3",
   "metadata": {
    "language": "sql",
    "name": "sql__prepare_demo",
    "collapsed": true,
    "codeCollapsed": true
   },
   "outputs": [],
   "source": "use schema \"DEMO\";",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "4566cba3-d0b3-49bd-a4d7-61896bab7934",
   "metadata": {
    "language": "python",
    "name": "py__prepare_demo",
    "collapsed": true,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "snowflake_session.sql(\"\"\"\n  create or replace stage \"STG__DATA\";\n\"\"\").collect()\n\nsnowflake_session.file.put(\"data/1.csv.gz\", '@\"STG__DATA\"/csv', overwrite=True)\nsnowflake_session.file.put(\"data/2.json.gz\", '@\"STG__DATA\"/json', overwrite=True)\nsnowflake_session.file.put(\"data/3.parquet\", '@\"STG__DATA\"/parquet', overwrite=True)\n\nst.dataframe(\n  snowflake_session.sql(\"\"\"\n    list @\"STG__DATA\";\n  \"\"\").collect()\n)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "08e0a760-eb55-4c3d-9933-fe7f7aa054ac",
   "metadata": {
    "language": "python",
    "name": "py_repeated_funcs",
    "collapsed": false,
    "codeCollapsed": true
   },
   "outputs": [],
   "source": "# Define function to print the set of shareable QR codes\ndef print_qr_codes():\n  qr_col_1, qr_col_2, qr_col_3 = st.columns(3, gap=\"large\")\n  with qr_col_1:\n    st.subheader(\"View this repository\")\n    st.image(\"images/QR-repo.png\", caption=\"This repository\")\n    st.markdown(\"[https://github.com/InterWorks/Snowflake-Build-2024---Schema-Evolution](https://github.com/InterWorks/Snowflake-Build-2024---Schema-Evolution)\")\n  with qr_col_2:\n    st.subheader(\"View my company profile\")\n    st.image(\"images/QR-IW-profile.png\", caption=\"My company profile\")\n    st.markdown(\"[https://interworks.com/people/chris-hastie](https://interworks.com/people/chris-hastie)\")\n  with qr_col_3:\n    st.subheader(\"View my LinkedIn profile\")\n    st.image(\"images/QR-LinkedIn.png\", caption=\"My LinkedIn profile\")\n    st.markdown(\"[https://www.linkedin.com/in/chris-hastie/](https://www.linkedin.com/in/chris-hastie/)\")",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "44759915-c5fa-44ec-9cb8-2b07acd753d3",
   "metadata": {
    "language": "python",
    "name": "py__intro",
    "collapsed": false,
    "codeCollapsed": true
   },
   "outputs": [],
   "source": "st.markdown(\"\"\"\n  # Schema Evolution for Automated Metadata-Driven Ingestion\n\n  In this session, learn about multiple features in Snowflake for automated data ingestion, using data from multiple file formats. We will:\n  \n    1. Leverage Snowflake's schema inference functionality to parse the metadata from the various different file formats\n    2. Demonstrate several ways that this metadata can be used\n    3. Ingest the data into new tables using Snowflake’s column-matching ingestion functionality\n\n  The icing on the cake will then be a **demonstration of Snowflake’s automated schema evolution functionality**, which supports changing workloads as new fields are added to landing tables automatically.\n  \n  Within all of this, we will also demonstrate how to parse and store a variety of other pieces of metadata during ingestion using metadata columns.\n\"\"\")\n\nprint_qr_codes()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "1ce93bd3-7da7-417a-8882-8a0ce0c918eb",
   "metadata": {
    "language": "python",
    "name": "py__file_formats__csv",
    "collapsed": false,
    "codeCollapsed": true
   },
   "outputs": [],
   "source": "st.header(\"File Formats - CSV\")\n\nst.markdown(\"\"\"\n  The typical file format that we're all familiar with.\n  \n  For our example, we have a pipe-delimited file of some basic event logging.\n\"\"\")\n\nfile_path__csv = r\"data/1.csv.gz\"\nwith gzip.open(file_path__csv, \"rt\") as csv_file:\n  st.code(csv_file.read(), language=\"csv\")\n\nst.markdown(\"\"\"\n  The file format here is fairly simple, however we leverage two options that are less standard:\n\n    - parse_header: Key option that ensures the first header is leveraged as a header when parsing metadata\n    - error_on_column_count_mismatch: Option that allows files to have missing/new columns compared to the destination table\n\"\"\")",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "8d50cbf4-0c8d-4950-86cb-114990437ac9",
   "metadata": {
    "language": "sql",
    "name": "sql__file_formats__csv",
    "codeCollapsed": false
   },
   "source": "create or replace file format \"FF_CSV\"\n  type = CSV\n  field_delimiter = '|'\n  parse_header = TRUE\n  error_on_column_count_mismatch = FALSE",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "c695373e-ac74-4b62-a1f1-08206cbd5c81",
   "metadata": {
    "language": "python",
    "name": "py__file_formats__json",
    "collapsed": false,
    "codeCollapsed": true
   },
   "source": "st.header(\"File Formats - JSON\")\n\nst.markdown(\"\"\"\n  The typical semi-structured file format that most of us are familiar with.\n  \n  For our example, we have a file with some more basic event logging.\n\"\"\")\n\nfile_path__json = r\"data/2.json.gz\"\nwith gzip.open(file_path__json, \"rt\") as json_file:\n  json_string = json_file.read()\n  json_data = json.loads(json_string)\n  json_pretty = json.dumps(json_data, indent = 2)\n    \nst.code(json_pretty, language=\"json\")\n\nst.markdown(\"\"\"\n  The file format here is more simple than for CSVs, however we leverage one options that is worth explaining:\n\n    - strip_outer_array: Reads each element as its own record instead of reading the entire file into a single record\n\"\"\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "48715241-a599-42bf-9537-357d2a046f75",
   "metadata": {
    "language": "sql",
    "name": "sql__file_formats__json"
   },
   "outputs": [],
   "source": "create or replace file format \"FF_JSON\"\n  type = JSON\n  strip_outer_array = TRUE",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "cab06e67-5293-4204-aac7-f05c823633db",
   "metadata": {
    "language": "python",
    "name": "py__file_formats__parquet",
    "collapsed": false,
    "codeCollapsed": true
   },
   "outputs": [],
   "source": "st.header(\"File Formats - Parquet\")\n\nst.markdown(\"\"\"\n  The common optimised semi-structured file format that many of us are familiar with.\n  \n  For our example, we have a file with some more basic event logging.\n\"\"\")\n\nfile_path__parquet = r\"data/3.parquet\"\nparquet_data = pq.read_table(file_path__parquet)\n    \nst.code(parquet_data, language=\"parquet\")\n\nst.markdown(\"The file format here is the most simple of our examples\")",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "964b20aa-6591-4798-841d-2887b3f6da5c",
   "metadata": {
    "language": "sql",
    "name": "sql__file_formats__parquet"
   },
   "outputs": [],
   "source": "create or replace file format \"FF_PARQUET\"\n  type = PARQUET",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "bdde1b6b-1663-40f9-bf15-96f8c85480a4",
   "metadata": {
    "language": "python",
    "name": "py__file_formats__summary",
    "collapsed": false,
    "codeCollapsed": true
   },
   "outputs": [],
   "source": "st.header(\"File Formats - Summary\")\n\nst.markdown(\"So we have three file formats:\")\n\nff_col_csv, ff_col_json, ff_col_parquet = st.columns(3, gap=\"large\")\nwith ff_col_csv:\n  st.subheader(\"CSV\")\n  st.code(sql__file_formats__csv.__getattribute__(\"query_executed\"), \"sql\")\nwith ff_col_json:\n  st.subheader(\"JSON\")\n  st.code(sql__file_formats__json.__getattribute__(\"query_executed\"), \"sql\")\nwith ff_col_parquet:\n  st.subheader(\"Parquet\")\n  st.code(sql__file_formats__parquet.__getattribute__(\"query_executed\"), \"sql\")",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "6db1a7a9-5e0e-4c8c-87fe-9cf4eebba9d4",
   "metadata": {
    "language": "python",
    "name": "py__inferring_schemas_directly"
   },
   "outputs": [],
   "source": "st.header(\"Inferring schemas directly\")\n\nst.markdown(\"\"\"\n  These files were all uploaded into an internal stage earlier, so we can dive straight into inferring the schema.\n\n  Inferring a schema from a file directly is achieved using a table function. This uses a very similar structure regardless of the file format:\n\"\"\")\n\nst.code(\"\"\"\n  table(\n    infer_schema(\n        location => '<location of file(s), including stage>'\n      , file_format => '<Snowflake File Format object to use when reading the file>'\n    )\n  )\n\"\"\", language=\"sql\")\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "76753f18-9780-4f77-a004-054a5410fa54",
   "metadata": {
    "language": "python",
    "name": "py__inferring_schemas_directly__csv",
    "collapsed": false,
    "codeCollapsed": true
   },
   "outputs": [],
   "source": "st.header(\"Inferring schemas directly - CSV\")\n\nst.markdown(\"Let's quickly see the output from this table function using our example CSV file:\")",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "1404bf3b-4c6c-402d-9e27-545de1b049a5",
   "metadata": {
    "language": "sql",
    "name": "sql__inferring_schemas_directly__csv"
   },
   "outputs": [],
   "source": "select *\nfrom table(\n  infer_schema(\n      location => '@\"STG__DATA\"/csv'\n    , file_format => '\"FF_CSV\"'\n  )\n)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b9b6425e-de95-45f3-9d82-747677b6813f",
   "metadata": {
    "language": "python",
    "name": "py__inferring_schemas_directly__json",
    "collapsed": false,
    "codeCollapsed": true
   },
   "outputs": [],
   "source": "st.header(\"Inferring schemas directly - JSON\")\n\nst.markdown(\"And now let's see the output from this table function using our example JSON file:\")",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "5fefdf69-b989-4fcc-86ae-9a91ebfb72be",
   "metadata": {
    "language": "sql",
    "name": "sql__inferring_schemas_directly__json"
   },
   "outputs": [],
   "source": "select *\nfrom table(\n  infer_schema(\n      location => '@\"STG__DATA\"/json'\n    , file_format => '\"FF_JSON\"'\n  )\n)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c38ea1c4-562d-42f3-8aec-a4e57d1c42dc",
   "metadata": {
    "language": "python",
    "name": "py__inferring_schemas_directly__parquet",
    "collapsed": false,
    "codeCollapsed": true
   },
   "outputs": [],
   "source": "st.header(\"Inferring schemas directly - Parquet\")\n\nst.markdown(\"Finally, let's see the output from this table function using our example Parquet file:\")",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "be8cb034-0e8e-4864-877c-d895917b1390",
   "metadata": {
    "language": "sql",
    "name": "sql__inferring_schemas_directly__parquet"
   },
   "outputs": [],
   "source": "select *\nfrom table(\n  infer_schema(\n      location => '@\"STG__DATA\"/parquet'\n    , file_format => '\"FF_PARQUET\"'\n  )\n)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "82523889-0e0e-42b2-be23-a6bad93b5464",
   "metadata": {
    "language": "python",
    "name": "py__inferring_schemas_directly__comparison"
   },
   "outputs": [],
   "source": "st.header(\"Inferring schemas directly - Summary\")\n\nst.markdown(\"So we have three very similar queries:\")\n\nisd_col_csv, isd_col_json, isd_col_parquet = st.columns(3, gap=\"large\")\nwith isd_col_csv:\n  st.subheader(\"CSV\")\n  st.code(sql__inferring_schemas_directly__csv.__getattribute__(\"query_executed\"), \"sql\")\nwith isd_col_json:\n  st.subheader(\"JSON\")\n  st.code(sql__inferring_schemas_directly__json.__getattribute__(\"query_executed\"), \"sql\")\nwith isd_col_parquet:\n  st.subheader(\"Parquet\")\n  st.code(sql__inferring_schemas_directly__parquet.__getattribute__(\"query_executed\"), \"sql\")\n\nst.markdown(\"\"\"\n  Each of these queries outputs a similar result, however there are some important differences:\n\"\"\")",
   "execution_count": null
  }
 ]
}