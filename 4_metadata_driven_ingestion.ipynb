{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3775908f-ca36-4846-8f38-5adca39217f2",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "py__imports"
   },
   "outputs": [],
   "source": [
    "# Import python packages\n",
    "import streamlit as st\n",
    "\n",
    "# Import Snowpark packages\n",
    "from snowflake.snowpark.context import get_active_session\n",
    "snowflake_session = get_active_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e10807-c07e-4e97-8eb1-64ac59409bb3",
   "metadata": {
    "codeCollapsed": true,
    "collapsed": true,
    "language": "sql",
    "name": "sql__prepare_demo"
   },
   "outputs": [],
   "source": [
    "use schema \"DEMO\";"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae6c516-723d-466c-81fa-13fa8e0752e6",
   "metadata": {
    "collapsed": false,
    "name": "docs__metadata_driven_ingestion"
   },
   "source": [
    "# Metadata-driven ingestion\n",
    "\n",
    "We now have all the tools we need to ingest data into Snowflake without manually defining any metadata.\n",
    "\n",
    "First, these table templates can be used directly inside a `create table` statement:\n",
    "\n",
    "```sql\n",
    "create or replace table \"MY_TABLE\"\n",
    "  using template(\n",
    "    select array_agg(object_construct(*))\n",
    "    from table(\n",
    "      infer_schema(\n",
    "          location => '<location of file(s), including stage>'\n",
    "        , file_format => '<Snowflake File Format object to use when reading the file>'\n",
    "      )\n",
    "    )\n",
    "  )\n",
    "  comment = 'Table created using the metadata inferred from the source file(s)'\n",
    "```\n",
    "\n",
    "Once we have created our landing tables (whether using a template or by manually defining appropriate fields), data can be ingested:\n",
    "\n",
    "```sql\n",
    "copy into table \"MY_TABLE\"\n",
    "from '@<location of file(s), including stage>'\n",
    "  file_format = \"<Snowflake File Format object to use when reading the file>\"\n",
    "  match_by_column_name = CASE_INSENSITIVE\n",
    "```\n",
    "\n",
    "The key option here is \"match_by_column_name\":\n",
    "\n",
    "- Removes the need for fields in the file to be in the same order as in the destination table\n",
    "- Most useful for CSVs where field order used to be far more important\n",
    "- Allows files to be ingested even if they are missing fields"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bcb9842-050e-429c-b3a7-310c14cef24f",
   "metadata": {
    "collapsed": false,
    "name": "docs__create_tables__csv"
   },
   "source": [
    "## Metadata-driven ingestion - CSV\n",
    "\n",
    "Let's quickly create a table using the table template from our example CSV file, then ingest the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07b4575-394e-437e-ba25-bf4f2cfbcaed",
   "metadata": {
    "language": "sql",
    "name": "sql__create_tables__csv"
   },
   "outputs": [],
   "source": [
    "create or replace table \"DATA_FROM_CSV\"\n",
    "  using template(\n",
    "    select array_agg(object_construct(*))\n",
    "    from table(\n",
    "      infer_schema(\n",
    "          location => '@\"STG__DATA\"/csv'\n",
    "        , file_format => '\"FF_CSV\"'\n",
    "      )\n",
    "    )\n",
    "  )\n",
    "  comment = 'Table created using the metadata inferred from the source file(s) in CSV format'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a9d5f8-cadc-4e4e-9a71-472d273f0a5b",
   "metadata": {
    "collapsed": false,
    "name": "docs__create_tables__json"
   },
   "source": [
    "## Metadata-driven ingestion - JSON\n",
    "\n",
    "And now let's create a table using the table template from our example JSON file, then ingest the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0a4f74-0667-44be-8bc5-b6d66df51c2d",
   "metadata": {
    "language": "sql",
    "name": "sql__create_tables__json"
   },
   "outputs": [],
   "source": [
    "create or replace table \"DATA_FROM_JSON\"\n",
    "  using template(\n",
    "    select array_agg(object_construct(*))\n",
    "    from table(\n",
    "      infer_schema(\n",
    "          location => '@\"STG__DATA\"/json'\n",
    "        , file_format => '\"FF_JSON\"'\n",
    "      )\n",
    "    )\n",
    "  )\n",
    "  comment = 'Table created using the metadata inferred from the source file(s) in JSON format'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aab2abd-7e24-4979-8dcd-76d72cbc74a3",
   "metadata": {
    "collapsed": false,
    "name": "docs__create_tables__parquet"
   },
   "source": [
    "## Metadata-driven ingestion - Parquet\n",
    "\n",
    "Finally, let's create a table using the table template from our example Parquet file, then ingest the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee6a1b21-6a23-4ad7-92c8-c4c3948384b2",
   "metadata": {
    "language": "sql",
    "name": "sql__create_tables__parquet"
   },
   "outputs": [],
   "source": [
    "create or replace table \"DATA_FROM_PARQUET\"\n",
    "  using template(\n",
    "    select array_agg(object_construct(*))\n",
    "    from table(\n",
    "      infer_schema(\n",
    "          location => '@\"STG__DATA\"/parquet'\n",
    "        , file_format => '\"FF_PARQUET\"'\n",
    "      )\n",
    "    )\n",
    "  )\n",
    "  comment = 'Table created using the metadata inferred from the source file(s) in Parquet format'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
