{
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "id": "3775908f-ca36-4846-8f38-5adca39217f2",
   "metadata": {
    "language": "python",
    "name": "py__imports",
    "collapsed": false,
    "codeCollapsed": false
   },
   "source": "# Import python packages\nimport streamlit as st\nimport gzip\nimport json\nimport pyarrow.parquet as pq\n\n# Import Snowpark packages\nfrom snowflake.snowpark.context import get_active_session\nsnowflake_session = get_active_session()",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "46e10807-c07e-4e97-8eb1-64ac59409bb3",
   "metadata": {
    "language": "sql",
    "name": "sql__prepare_demo",
    "collapsed": true,
    "codeCollapsed": true
   },
   "outputs": [],
   "source": "use schema \"DEMO\";",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "4566cba3-d0b3-49bd-a4d7-61896bab7934",
   "metadata": {
    "language": "python",
    "name": "py__prepare_demo",
    "collapsed": true,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "snowflake_session.sql(\"\"\"\n  create or replace stage \"STG__DATA\";\n\"\"\").collect()\n\nsnowflake_session.file.put(\"data/1.csv.gz\", '@\"STG__DATA\"/csv', overwrite=True)\nsnowflake_session.file.put(\"data/2.json.gz\", '@\"STG__DATA\"/json', overwrite=True)\nsnowflake_session.file.put(\"data/3.parquet\", '@\"STG__DATA\"/parquet', overwrite=True)\n\nst.dataframe(\n  snowflake_session.sql(\"\"\"\n    list @\"STG__DATA\";\n  \"\"\").collect()\n)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "08e0a760-eb55-4c3d-9933-fe7f7aa054ac",
   "metadata": {
    "language": "python",
    "name": "py_repeated_funcs",
    "collapsed": false,
    "codeCollapsed": true
   },
   "outputs": [],
   "source": "# Define function to print the set of shareable QR codes\ndef print_qr_codes():\n  qr_col_1, qr_col_2, qr_col_3 = st.columns(3, gap=\"large\")\n  with qr_col_1:\n    st.subheader(\"View this repository\")\n    st.image(\"images/QR-repo.png\", caption=\"This repository\")\n    st.markdown(\"[https://github.com/InterWorks/Snowflake-Build-2024---Schema-Evolution](https://github.com/InterWorks/Snowflake-Build-2024---Schema-Evolution)\")\n  with qr_col_2:\n    st.subheader(\"View my company profile\")\n    st.image(\"images/QR-IW-profile.png\", caption=\"My company profile\")\n    st.markdown(\"[https://interworks.com/people/chris-hastie](https://interworks.com/people/chris-hastie)\")\n  with qr_col_3:\n    st.subheader(\"View my LinkedIn profile\")\n    st.image(\"images/QR-LinkedIn.png\", caption=\"My LinkedIn profile\")\n    st.markdown(\"[https://www.linkedin.com/in/chris-hastie/](https://www.linkedin.com/in/chris-hastie/)\")",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "44759915-c5fa-44ec-9cb8-2b07acd753d3",
   "metadata": {
    "language": "python",
    "name": "py__intro",
    "collapsed": false,
    "codeCollapsed": true
   },
   "outputs": [],
   "source": "st.markdown(\"\"\"\n  # Schema Evolution for Automated Metadata-Driven Ingestion\n\n  In this session, learn about multiple features in Snowflake for automated data ingestion, using data from multiple file formats. We will:\n  \n    1. Leverage Snowflake's schema inference functionality to parse the metadata from the various different file formats\n    2. Demonstrate several ways that this metadata can be used\n    3. Ingest the data into new tables using Snowflake’s column-matching ingestion functionality\n\n  The icing on the cake will then be a **demonstration of Snowflake’s automated schema evolution functionality**, which supports changing workloads as new fields are added to landing tables automatically.\n  \n  Within all of this, we will also demonstrate how to parse and store a variety of other pieces of metadata during ingestion using metadata columns.\n\"\"\")\n\nprint_qr_codes()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "1ce93bd3-7da7-417a-8882-8a0ce0c918eb",
   "metadata": {
    "language": "python",
    "name": "py__file_formats__csv",
    "collapsed": false,
    "codeCollapsed": true
   },
   "outputs": [],
   "source": "st.header(\"File Formats - CSV\")\n\nst.markdown(\"\"\"\n  The typical file format that we're all familiar with.\n  \n  For our example, we have a pipe-delimited file of some basic event logging.\n\"\"\")\n\nfile_path__csv = r\"data/1.csv.gz\"\nwith gzip.open(file_path__csv, \"rt\") as csv_file:\n  st.code(csv_file.read(), language=\"csv\")\n\nst.markdown(\"\"\"\n  The file format here is fairly simple, however we leverage two options that are less standard:\n\n    - parse_header: Key option that ensures the first header is leveraged as a header when parsing metadata\n    - error_on_column_count_mismatch: Option that allows files to have missing/new columns compared to the destination table\n\"\"\")",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "8d50cbf4-0c8d-4950-86cb-114990437ac9",
   "metadata": {
    "language": "sql",
    "name": "sql__file_formats__csv",
    "codeCollapsed": false
   },
   "source": "create or replace file format \"FF_CSV\"\n  type = CSV\n  field_delimiter = '|'\n  parse_header = TRUE\n  error_on_column_count_mismatch = FALSE",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "c695373e-ac74-4b62-a1f1-08206cbd5c81",
   "metadata": {
    "language": "python",
    "name": "py__file_formats__json",
    "collapsed": false,
    "codeCollapsed": true
   },
   "source": "st.header(\"File Formats - JSON\")\n\nst.markdown(\"\"\"\n  The typical semi-structured file format that most of us are familiar with.\n  \n  For our example, we have a file with some more basic event logging.\n\"\"\")\n\nfile_path__json = r\"data/2.json.gz\"\nwith gzip.open(file_path__json, \"rt\") as json_file:\n  json_string = json_file.read()\n  json_data = json.loads(json_string)\n  json_pretty = json.dumps(json_data, indent = 2)\n    \nst.code(json_pretty, language=\"json\")\n\nst.markdown(\"\"\"\n  The file format here is more simple than for CSVs, however we leverage one options that is worth explaining:\n\n    - strip_outer_array: Reads each element as its own record instead of reading the entire file into a single record\n\"\"\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "48715241-a599-42bf-9537-357d2a046f75",
   "metadata": {
    "language": "sql",
    "name": "sql__file_formats__json"
   },
   "outputs": [],
   "source": "create or replace file format \"FF_JSON\"\n  type = JSON\n  strip_outer_array = TRUE",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "cab06e67-5293-4204-aac7-f05c823633db",
   "metadata": {
    "language": "python",
    "name": "py__file_formats__parquet",
    "collapsed": false,
    "codeCollapsed": true
   },
   "outputs": [],
   "source": "st.header(\"File Formats - Parquet\")\n\nst.markdown(\"\"\"\n  The common optimised semi-structured file format that many of us are familiar with.\n  \n  For our example, we have a file with some more basic event logging.\n\"\"\")\n\nfile_path__parquet = r\"data/3.parquet\"\nparquet_data = pq.read_table(file_path__parquet)\n    \nst.code(parquet_data, language=\"parquet\")\n\nst.markdown(\"The file format here is the most simple of our examples\")",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "964b20aa-6591-4798-841d-2887b3f6da5c",
   "metadata": {
    "language": "sql",
    "name": "sql__file_formats__parquet"
   },
   "outputs": [],
   "source": "create or replace file format \"FF_PARQUET\"\n  type = PARQUET",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "bdde1b6b-1663-40f9-bf15-96f8c85480a4",
   "metadata": {
    "language": "python",
    "name": "py__file_formats__summary",
    "collapsed": false,
    "codeCollapsed": true
   },
   "outputs": [],
   "source": "st.header(\"File Formats - Summary\")\n\nst.markdown(\"So we have three file formats:\")\n\nff_col_csv, ff_col_json, ff_col_parquet = st.columns(3, gap=\"large\")\nwith ff_col_csv:\n  st.subheader(\"CSV\")\n  st.code(sql__file_formats__csv.__getattribute__(\"query_executed\"), \"sql\")\nwith ff_col_json:\n  st.subheader(\"JSON\")\n  st.code(sql__file_formats__json.__getattribute__(\"query_executed\"), \"sql\")\nwith ff_col_parquet:\n  st.subheader(\"Parquet\")\n  st.code(sql__file_formats__parquet.__getattribute__(\"query_executed\"), \"sql\")",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "6db1a7a9-5e0e-4c8c-87fe-9cf4eebba9d4",
   "metadata": {
    "name": "docs__inferring_schemas_directly",
    "collapsed": false
   },
   "source": "# Inferring schemas directly\n\nThese files were all uploaded into an internal stage earlier, so we can dive straight into inferring the schema.\n\nInferring a schema from a file directly is achieved using a table function. This uses a very similar structure regardless of the file format:\n\n```sql\ntable(\n  infer_schema(\n      location => '<location of file(s), including stage>'\n    , file_format => '<Snowflake File Format object to use when reading the file>'\n  )\n)\n```\n"
  },
  {
   "cell_type": "markdown",
   "id": "76753f18-9780-4f77-a004-054a5410fa54",
   "metadata": {
    "name": "docs__inferring_schemas_directly__csv",
    "collapsed": false
   },
   "source": "## Inferring schemas directly - CSV\n\nLet's quickly see the output from this table function using our example CSV file:"
  },
  {
   "cell_type": "code",
   "id": "1404bf3b-4c6c-402d-9e27-545de1b049a5",
   "metadata": {
    "language": "sql",
    "name": "sql__inferring_schemas_directly__csv"
   },
   "outputs": [],
   "source": "select *\nfrom table(\n  infer_schema(\n      location => '@\"STG__DATA\"/csv'\n    , file_format => '\"FF_CSV\"'\n  )\n)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "b9b6425e-de95-45f3-9d82-747677b6813f",
   "metadata": {
    "name": "docs__inferring_schemas_directly__json",
    "collapsed": false
   },
   "source": "## Inferring schemas directly - JSON\n\nAnd now let's see the output from this table function using our example JSON file:"
  },
  {
   "cell_type": "code",
   "id": "5fefdf69-b989-4fcc-86ae-9a91ebfb72be",
   "metadata": {
    "language": "sql",
    "name": "sql__inferring_schemas_directly__json",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "select *\nfrom table(\n  infer_schema(\n      location => '@\"STG__DATA\"/json'\n    , file_format => '\"FF_JSON\"'\n  )\n)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "c38ea1c4-562d-42f3-8aec-a4e57d1c42dc",
   "metadata": {
    "name": "docs__inferring_schemas_directly__parquet",
    "collapsed": false
   },
   "source": "## Inferring schemas directly - Parquet\n\nFinally, let's see the output from this table function using our example Parquet file:"
  },
  {
   "cell_type": "code",
   "id": "be8cb034-0e8e-4864-877c-d895917b1390",
   "metadata": {
    "language": "sql",
    "name": "sql__inferring_schemas_directly__parquet",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "select *\nfrom table(\n  infer_schema(\n      location => '@\"STG__DATA\"/parquet'\n    , file_format => '\"FF_PARQUET\"'\n  )\n)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "82523889-0e0e-42b2-be23-a6bad93b5464",
   "metadata": {
    "language": "python",
    "name": "py__inferring_schemas_directly__comparison",
    "collapsed": false,
    "codeCollapsed": true
   },
   "outputs": [],
   "source": "st.header(\"Inferring schemas directly - Summary\")\n\nst.markdown(\"So we have three very similar queries that output similar results, however there are some important differences:\")\n\nisd_col_csv, isd_col_json, isd_col_parquet = st.columns(3, gap=\"large\")\nwith isd_col_csv:\n  st.subheader(\"CSV\")\n  st.code(sql__inferring_schemas_directly__csv.__getattribute__(\"query_executed\"), \"sql\")\n  st.markdown(\"\"\"\n    Metadata inferred from CSV data is the least reliable\n      - Number accuracy is estimated\n      - Strings may incorrectly be inferred as other types\n  \"\"\")\nwith isd_col_json:\n  st.subheader(\"JSON\")\n  st.code(sql__inferring_schemas_directly__json.__getattribute__(\"query_executed\"), \"sql\")\n  st.markdown(\"\"\"\n    Metadata inferred from JSON data is still not optimal\n      - Number accuracy is estimated\n  \"\"\")\nwith isd_col_parquet:\n  st.subheader(\"Parquet\")\n  st.code(sql__inferring_schemas_directly__parquet.__getattribute__(\"query_executed\"), \"sql\")\n  st.markdown(\"\"\"\n    Metadata inferred from Parquet files is exact\n      - Metadata is stored within the file format\n  \"\"\")\n",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "ba0c1675-b04e-40d5-852b-fe3cee7e61fc",
   "metadata": {
    "name": "docs__table_templates",
    "collapsed": false
   },
   "source": "# Table templates\n\nThe output of the table function used to infer metadata from a file can be leveraged to create dictionaries that contain all the metadata for a given file.\n\n```sql\nselect array_agg(object_construct(*))\ntable(\n  infer_schema(\n      location => '<location of file(s), including stage>'\n    , file_format => '<Snowflake File Format object to use when reading the file>'\n  )\n)\n```\n"
  },
  {
   "cell_type": "markdown",
   "id": "c9cee1a0-3f10-4297-8a54-e4b4c64f15ca",
   "metadata": {
    "name": "docs__table_templates__csv",
    "collapsed": false
   },
   "source": "## Table templates - CSV\n\nLet's quickly see the table template from our example CSV file:"
  },
  {
   "cell_type": "code",
   "id": "4594e300-f17b-4682-9f9b-24376631b128",
   "metadata": {
    "language": "sql",
    "name": "sql__table_templates__csv"
   },
   "outputs": [],
   "source": "select array_agg(object_construct(*))\nfrom table(\n  infer_schema(\n      location => '@\"STG__DATA\"/csv'\n    , file_format => '\"FF_CSV\"'\n  )\n)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "7ebcc49c-9fca-4fe7-9ed0-4e2cecd43f5b",
   "metadata": {
    "name": "docs__table_templates__json",
    "collapsed": false
   },
   "source": "## Table templates - JSON\n\nAnd now let's see the table template from our example JSON file:"
  },
  {
   "cell_type": "code",
   "id": "4bfb1c70-6750-4753-b769-6567d3520559",
   "metadata": {
    "language": "sql",
    "name": "sql__table_templates__json"
   },
   "outputs": [],
   "source": "select array_agg(object_construct(*))\nfrom table(\n  infer_schema(\n      location => '@\"STG__DATA\"/json'\n    , file_format => '\"FF_JSON\"'\n  )\n)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "757686c4-af53-4278-86eb-51973d815451",
   "metadata": {
    "name": "docs__table_templates__parquet",
    "collapsed": false
   },
   "source": "## Table templates - Parquet\n\nFinally, let's see the table template from our example Parquet file:"
  },
  {
   "cell_type": "code",
   "id": "bd9187b0-d4da-4dd5-be03-1b901222edca",
   "metadata": {
    "language": "sql",
    "name": "sql__table_templates__parquet",
    "collapsed": false
   },
   "outputs": [],
   "source": "select array_agg(object_construct(*))\nfrom table(\n  infer_schema(\n      location => '@\"STG__DATA\"/parquet'\n    , file_format => '\"FF_PARQUET\"'\n  )\n)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "aa2fd253-c2b2-4e24-a392-2a52f3b5658d",
   "metadata": {
    "language": "python",
    "name": "py__table_templates__comparison",
    "collapsed": false,
    "codeCollapsed": true
   },
   "outputs": [],
   "source": "st.header(\"Table templates - Summary\")\n\nst.markdown(\"Again, we have three very similar queries that output similar results:\")\n\ntt_col_csv, tt_col_json, tt_col_parquet = st.columns(3, gap=\"large\")\nwith tt_col_csv:\n  st.subheader(\"CSV\")\n  st.code(sql__table_templates__csv.__getattribute__(\"query_executed\"), \"sql\")\n  st.code(\n      json.dumps(\n          json.loads(\n            sql__table_templates__csv.__getattribute__(\"results\").to_dict(\"records\")[0][\"ARRAY_AGG(OBJECT_CONSTRUCT(*))\"]\n          )\n        , indent = 2\n      )\n    , \"json\"\n  )\nwith tt_col_json:\n  st.subheader(\"JSON\")\n  st.code(sql__table_templates__json.__getattribute__(\"query_executed\"), \"sql\")\n  st.code(\n      json.dumps(\n          json.loads(\n            sql__table_templates__json.__getattribute__(\"results\").to_dict(\"records\")[0][\"ARRAY_AGG(OBJECT_CONSTRUCT(*))\"]\n          )\n        , indent = 2\n      )\n    , \"json\"\n  )\nwith tt_col_parquet:\n  st.subheader(\"Parquet\")\n  st.code(sql__table_templates__parquet.__getattribute__(\"query_executed\"), \"sql\")\n  st.code(\n      json.dumps(\n          json.loads(\n            sql__table_templates__parquet.__getattribute__(\"results\").to_dict(\"records\")[0][\"ARRAY_AGG(OBJECT_CONSTRUCT(*))\"]\n          )\n        , indent = 2\n      )\n    , \"json\"\n  )",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "d8c9a094-9647-483d-9beb-77020ffc2691",
   "metadata": {
    "name": "docs__create_tables",
    "collapsed": false
   },
   "source": "# Create tables\n\nThese table templates can be used directly inside a `create table` statement:\n\n```sql\ncreate or replace table \"MY_TABLE\"\n  using template(\n    select array_agg(object_construct(*))\n    from table(\n      infer_schema(\n          location => '<location of file(s), including stage>'\n        , file_format => '<Snowflake File Format object to use when reading the file>'\n      )\n    )\n  )\n  comment = 'Table created using the metadata inferred from the source file(s)'\n```\n"
  },
  {
   "cell_type": "markdown",
   "id": "6bcb9842-050e-429c-b3a7-310c14cef24f",
   "metadata": {
    "name": "docs__create_tables__csv",
    "collapsed": false
   },
   "source": "## Create table - CSV\n\nLet's quickly create a table using the table template from our CSV file:"
  },
  {
   "cell_type": "code",
   "id": "e07b4575-394e-437e-ba25-bf4f2cfbcaed",
   "metadata": {
    "language": "sql",
    "name": "sql__create_tables__csv"
   },
   "outputs": [],
   "source": "create or replace table \"DATA_FROM_CSV\"\n  using template(\n    select array_agg(object_construct(*))\n    from table(\n      infer_schema(\n          location => '@\"STG__DATA\"/csv'\n        , file_format => '\"FF_CSV\"'\n      )\n    )\n  )\n  comment = 'Table created using the metadata inferred from the source file(s) in CSV format'",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "14a9d5f8-cadc-4e4e-9a71-472d273f0a5b",
   "metadata": {
    "name": "docs__create_tables__json",
    "collapsed": false
   },
   "source": "## Table templates - JSON\n\nAnd now let's create a table using the table template from our example JSON file:"
  },
  {
   "cell_type": "code",
   "id": "5f0a4f74-0667-44be-8bc5-b6d66df51c2d",
   "metadata": {
    "language": "sql",
    "name": "sql__create_tables__json"
   },
   "outputs": [],
   "source": "create or replace table \"DATA_FROM_JSON\"\n  using template(\n    select array_agg(object_construct(*))\n    from table(\n      infer_schema(\n          location => '@\"STG__DATA\"/json'\n        , file_format => '\"FF_JSON\"'\n      )\n    )\n  )\n  comment = 'Table created using the metadata inferred from the source file(s) in JSON format'",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "1aab2abd-7e24-4979-8dcd-76d72cbc74a3",
   "metadata": {
    "name": "docs__create_tables__parquet",
    "collapsed": false
   },
   "source": "## Table templates - Parquet\n\nFinally, let's create a table using the table template from our example Parquet file:"
  },
  {
   "cell_type": "code",
   "id": "ee6a1b21-6a23-4ad7-92c8-c4c3948384b2",
   "metadata": {
    "language": "sql",
    "name": "sql__create_tables__parquet"
   },
   "outputs": [],
   "source": "create or replace table \"DATA_FROM_PARQUET\"\n  using template(\n    select array_agg(object_construct(*))\n    from table(\n      infer_schema(\n          location => '@\"STG__DATA\"/parquet'\n        , file_format => '\"FF_PARQUET\"'\n      )\n    )\n  )\n  comment = 'Table created using the metadata inferred from the source file(s) in Parquet format'",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "9ae6c516-723d-466c-81fa-13fa8e0752e6",
   "metadata": {
    "name": "docs__metadata_driven_ingestion",
    "collapsed": false
   },
   "source": "# Metadata-driven ingestion\n\nOnce we have created our landing tables (whether using a template or by manually defining appropriate fields) data can be ingested:\n\n```sql\ncopy into table \"MY_TABLE\"\nfrom '@<location of file(s), including stage>'\n  file_format = \"<Snowflake File Format object to use when reading the file>\"\n  match_by_column_name = CASE_INSENSITIVE\n```\n\nThe key option here is \"match_by_column_name\":\n\n- Removes the need for fields in the file to be in the same order as in the destination table\n- Most useful for CSVs where field order used to be far more important\n- Allows files to be ingested even if they are missing fields"
  },
  {
   "cell_type": "markdown",
   "id": "d23eb1e8-6ebf-499a-b12d-5f9eb1455df7",
   "metadata": {
    "name": "docs__metadata_driven_ingestion__csv",
    "collapsed": false
   },
   "source": "## Create table - CSV\n\nLet's quickly create a table using the table template from our CSV file:"
  },
  {
   "cell_type": "markdown",
   "id": "9408703a-e30f-4110-a179-c508a2e5b494",
   "metadata": {
    "name": "docs__metadata_driven_ingestion__json",
    "collapsed": false
   },
   "source": "## Table templates - JSON\n\nAnd now let's create a table using the table template from our example JSON file:"
  },
  {
   "cell_type": "markdown",
   "id": "c2f0a02a-888f-4136-94b6-6040255b635f",
   "metadata": {
    "name": "docs__metadata_driven_ingestion__parquet",
    "collapsed": false
   },
   "source": "## Table templates - Parquet\n\nFinally, let's create a table using the table template from our example Parquet file:"
  }
 ]
}