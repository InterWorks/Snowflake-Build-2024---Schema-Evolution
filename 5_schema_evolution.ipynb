{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3775908f-ca36-4846-8f38-5adca39217f2",
   "metadata": {
    "codeCollapsed": true,
    "collapsed": false,
    "language": "python",
    "name": "py__imports"
   },
   "outputs": [],
   "source": [
    "# Import python packages\n",
    "import streamlit as st\n",
    "import json\n",
    "\n",
    "# Import Snowpark packages\n",
    "from snowflake.snowpark.context import get_active_session\n",
    "snowflake_session = get_active_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc5e614-ce41-4081-8b4d-8aa04a1da1a1",
   "metadata": {
    "language": "python",
    "name": "py__def__display_schema_evolution_records"
   },
   "outputs": [],
   "source": [
    "def display_schema_evolution_records(\n",
    "    snowflake_query_result\n",
    "  , source_type: str\n",
    "):\n",
    "  results_as_list = snowflake_query_result.__getattribute__(\"results\").to_dict(\"records\")\n",
    "  list_of_values = [\n",
    "    {\n",
    "        \"name\": x[\"name\"]\n",
    "      , \"schema_evolution_record\": json.loads(x[\"schema evolution record\"])\n",
    "    }\n",
    "    for x in results_as_list\n",
    "    if (\n",
    "          (x[\"schema evolution record\"] is not None)\n",
    "      and (json.loads(x[\"schema evolution record\"])[\"fileName\"].split(\"/\")[0] == source_type)\n",
    "    )\n",
    "  ]\n",
    "\n",
    "  cols = st.columns(len(list_of_values))\n",
    "\n",
    "  for i, col_x in enumerate(cols):\n",
    "    with col_x:\n",
    "      st.write(list_of_values[i][\"name\"])\n",
    "      st.code(\n",
    "        json.dumps(\n",
    "            list_of_values[i][\"schema_evolution_record\"]\n",
    "          , indent = 2\n",
    "        )\n",
    "      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e10807-c07e-4e97-8eb1-64ac59409bb3",
   "metadata": {
    "codeCollapsed": true,
    "collapsed": true,
    "language": "sql",
    "name": "sql__prepare_demo"
   },
   "outputs": [],
   "source": [
    "use schema \"DEMO\";"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e03eb4-491b-4bdb-a5d3-6056348b24e6",
   "metadata": {
    "collapsed": false,
    "name": "docs__schema_evolution"
   },
   "source": [
    "# Schema Evolution\n",
    "\n",
    "So far, we have established the following building blocks:\n",
    "\n",
    "- Infer metadata from a file\n",
    "- Leverage the metadata to create a new table\n",
    "- Ingest data into the new table, matching by column name\n",
    "\n",
    "Now for the main event in this session. Our next demonstration achieves the following:\n",
    "\n",
    "1. Create a new table that only contains some generic metadata fields for data lineage and monitoring\n",
    "2. Ingest our example CSV file into this new table, _automatically_ adding all the data fields\n",
    "3. Ingest our example JSON file into this new table, _automatically_ adding the additional fields\n",
    "4. Ingest our example Parquet file into this new table, _automatically_ adding the final additional field\n",
    "\n",
    "First, we quickly review which fields are available in our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0407bd9-48c9-49f9-ae4d-ead6924ce388",
   "metadata": {
    "codeCollapsed": true,
    "collapsed": false,
    "language": "sql",
    "name": "sql__review_fields_in_source_data"
   },
   "outputs": [],
   "source": [
    "with \"CTE__RAW\" as (\n",
    "  select\n",
    "      \"TABLE_NAME\"\n",
    "    , \"COLUMN_NAME\"\n",
    "    , True as \"PIVOT_VALUE\"\n",
    "    , ((count(distinct \"TABLE_NAME\") over (partition by \"COLUMN_NAME\")) != 3)::int as \"CUSTOM_ORDER_1\"\n",
    "    , case \"TABLE_NAME\"\n",
    "        when 'DATA_FROM_CSV' then 1\n",
    "        when 'DATA_FROM_JSON' then 2\n",
    "        when 'DATA_FROM_PARQUET' then 3\n",
    "      end as \"CUSTOM_ORDER_2\"\n",
    "    , \"ORDINAL_POSITION\" as \"CUSTOM_ORDER_3\"\n",
    "    , concat(\n",
    "          \"CUSTOM_ORDER_1\"\n",
    "        , \"CUSTOM_ORDER_2\"\n",
    "        , \"CUSTOM_ORDER_3\"\n",
    "      ) as \"CUSTOM_ORDER\"\n",
    "  from \"INFORMATION_SCHEMA\".\"COLUMNS\"\n",
    "  where \"TABLE_SCHEMA\" = 'DEMO'\n",
    "    and \"TABLE_NAME\" in (\n",
    "        'DATA_FROM_CSV'\n",
    "      , 'DATA_FROM_JSON'\n",
    "      , 'DATA_FROM_PARQUET'\n",
    "    )\n",
    ")\n",
    "select\n",
    "    \"COLUMN_NAME\"\n",
    "  , max(\"'DATA_FROM_CSV'\") as \"DATA_FROM_CSV\"\n",
    "  , max(\"'DATA_FROM_JSON'\") as \"DATA_FROM_JSON\"\n",
    "  , max(\"'DATA_FROM_PARQUET'\") as \"DATA_FROM_PARQUET\"\n",
    "from \"CTE__RAW\"\n",
    "  pivot(\n",
    "    max(\"PIVOT_VALUE\")\n",
    "    for \"TABLE_NAME\" in (\n",
    "        'DATA_FROM_CSV'\n",
    "      , 'DATA_FROM_JSON'\n",
    "      , 'DATA_FROM_PARQUET'\n",
    "    )\n",
    "  )\n",
    "group by \"COLUMN_NAME\"\n",
    "order by\n",
    "    min(\"CUSTOM_ORDER\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7da294b-048b-42ab-a500-d2de2754f16b",
   "metadata": {
    "collapsed": false,
    "name": "docs__create_destination_table"
   },
   "source": [
    "## Create the destination table\n",
    "\n",
    "We will be loading all data into a single table.\n",
    "\n",
    "We _could_ create the destination table using a table template from one of the sources as seen earlier in the session.\n",
    "\n",
    "Instead, we create a brand new table first so we can fully show the power of metadata-driven ingestion and schema evolution.\n",
    "\n",
    "The most important thing to note here is the `enable_schema_evolution` option, which instructs Snowflake to add new fields if required when ingesting data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af4d326-30f1-4846-8350-5a79beeaa726",
   "metadata": {
    "codeCollapsed": false,
    "language": "sql",
    "name": "sql__create_destination_table"
   },
   "outputs": [],
   "source": [
    "create or replace table \"DATA_FROM_ALL_SOURCES\"(\n",
    "    \"METADATA_FILE_PATH\"                    string          comment 'Full path for the file in the originating stage'\n",
    "  , \"METADATA_FILE_ROW_NUMBER\"              integer         comment 'Row number within the file in the originating stage'\n",
    "  , \"METADATA_RECORD_INGESTION_TIMESTAMP\"   timestamp_ltz   comment 'Timestamp of record ingestion in local timezone'\n",
    ")\n",
    "  enable_schema_evolution = TRUE\n",
    "  comment = 'Table containing data from all sources, with schema evolution enabled to automatically add new columns as required'\n",
    ";\n",
    "\n",
    "desc table \"DATA_FROM_ALL_SOURCES\";"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12152e4b-1f69-48af-844d-2b7480139278",
   "metadata": {
    "collapsed": false,
    "name": "docs__including_metadata"
   },
   "source": [
    "## Including metadata in ingestion\n",
    "\n",
    "Before the introduction of the `include_metadata` option to the `COPY INTO` fuctionality when ingesting data, the only way to include metadata fields was by including a subquery within the statement. Unfortunately, this functionality (at time of writing) does not support the `match_by_column_name` option.\n",
    "\n",
    "Fortunately, the `include_metadata` option allows us to directly map any [Metadata Columns](https://docs.snowflake.com/en/user-guide/querying-metadata#metadata-columns) to fields in the target table:\n",
    "\n",
    "```sql\n",
    "copy into \"MY_TABLE\"\n",
    "from '<location of file(s), including stage>'\n",
    "  file_format = \"<Snowflake File Format object to use when reading the file>\"\n",
    "  match_by_column_name = CASE_INSENSITIVE\n",
    "  include_metadata = (\n",
    "      \"METADATA_FILE_PATH\" = METADATA$FILENAME\n",
    "    , \"METADATA_FILE_ROW_NUMBER\" = METADATA$FILE_ROW_NUMBER\n",
    "    , \"METADATA_RECORD_INGESTION_TIMESTAMP\" = METADATA$START_SCAN_TIME\n",
    "  )\n",
    ";\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e2cf93-1a49-4571-bacf-33612548e9c7",
   "metadata": {
    "collapsed": false,
    "name": "docs__ingestion_with_evolution__csv"
   },
   "source": [
    "## Ingestion with evolution - CSV\n",
    "\n",
    "To start, we ingest our example CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da8ecfc-9424-4d29-8e8a-6e57d8bfc049",
   "metadata": {
    "collapsed": false,
    "language": "sql",
    "name": "sql__ingestion_with_evolution__csv"
   },
   "outputs": [],
   "source": [
    "copy into \"DATA_FROM_ALL_SOURCES\"\n",
    "from '@\"STG__DATA\"/csv'\n",
    "  file_format = \"FF_CSV\"\n",
    "  match_by_column_name = CASE_INSENSITIVE\n",
    "  include_metadata = (\n",
    "      \"METADATA_FILE_PATH\" = METADATA$FILENAME\n",
    "    , \"METADATA_FILE_ROW_NUMBER\" = METADATA$FILE_ROW_NUMBER\n",
    "    , \"METADATA_RECORD_INGESTION_TIMESTAMP\" = METADATA$START_SCAN_TIME\n",
    "  )\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df2cae4-3239-4f72-b8e2-0e1681053019",
   "metadata": {
    "collapsed": false,
    "name": "docs__ingestion_with_evolution__csv__metadata"
   },
   "source": [
    "We can now see the additional fields have been created in the target table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "279fcdad-ae8b-4a9f-85b3-2b27adf4b0ef",
   "metadata": {
    "language": "sql",
    "name": "sql__ingestion_with_evolution__csv__metadata"
   },
   "outputs": [],
   "source": [
    "desc table \"DATA_FROM_ALL_SOURCES\";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc81759-9bc9-4a16-97fb-6bf0f3ff74c2",
   "metadata": {
    "language": "python",
    "name": "py__ingestion_with_evolution__csv__metadata"
   },
   "outputs": [],
   "source": [
    "st.markdown(\"\"\"We can expand the \"schema evolution record\" in this output to see exactly where each field came from:\"\"\")\n",
    "\n",
    "display_schema_evolution_records(\n",
    "    snowflake_query_result = sql__ingestion_with_evolution__csv__metadata\n",
    "  , source_type = \"csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9fca19b-6fdc-494c-9b43-65d52cf19294",
   "metadata": {
    "collapsed": false,
    "name": "docs__ingestion_with_evolution__csv__data"
   },
   "source": [
    "We can query the table to see how these new fields have been populated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f70f047-57ef-4f1a-bb2d-2abd5a5c8b4e",
   "metadata": {
    "language": "sql",
    "name": "sql__ingestion_with_evolution__csv__data"
   },
   "outputs": [],
   "source": [
    "select * from \"DATA_FROM_ALL_SOURCES\";"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee29aa4-023d-46ae-80b3-8993ab89835d",
   "metadata": {
    "collapsed": false,
    "name": "docs__ingestion_with_evolution__json"
   },
   "source": [
    "## Ingestion with evolution - JSON\n",
    "\n",
    "Next, we ingest our example JSON file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7335cffc-b23d-4183-8787-471f2e685c11",
   "metadata": {
    "language": "sql",
    "name": "sql__ingestion_with_evolution__json"
   },
   "outputs": [],
   "source": [
    "copy into \"DATA_FROM_ALL_SOURCES\"\n",
    "from '@\"STG__DATA\"/json'\n",
    "  file_format = \"FF_JSON\"\n",
    "  match_by_column_name = CASE_INSENSITIVE\n",
    "  include_metadata = (\n",
    "      \"METADATA_FILE_PATH\" = METADATA$FILENAME\n",
    "    , \"METADATA_FILE_ROW_NUMBER\" = METADATA$FILE_ROW_NUMBER\n",
    "    , \"METADATA_RECORD_INGESTION_TIMESTAMP\" = METADATA$START_SCAN_TIME\n",
    "  )\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c1b9236-da3c-4e49-9e4b-a0b28afe9541",
   "metadata": {
    "collapsed": false,
    "name": "docs__ingestion_with_evolution__json__metadata"
   },
   "source": [
    "Again, we can now see the additional fields have been created in the target table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd47e9a-6f97-4545-8563-ee60e8c5ed74",
   "metadata": {
    "language": "sql",
    "name": "sql__ingestion_with_evolution__json__metadata"
   },
   "outputs": [],
   "source": [
    "desc table \"DATA_FROM_ALL_SOURCES\";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a069aea1-4f6e-472f-9b8c-a4d83b8fdb05",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "py__ingestion_with_evolution__json__metadata"
   },
   "outputs": [],
   "source": [
    "st.markdown(\"\"\"Again, we can expand the \"schema evolution record\" in this output to see exactly where each field came from:\"\"\")\n",
    "\n",
    "display_schema_evolution_records(\n",
    "    snowflake_query_result = sql__ingestion_with_evolution__json__metadata\n",
    "  , source_type = \"json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f6a7ab-cf7b-4754-ba5e-5e2dd3c66144",
   "metadata": {
    "collapsed": false,
    "name": "docs__ingestion_with_evolution__json__data"
   },
   "source": [
    "Again, we can query the table to see how these new fields have been populated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ca1cf2-9802-40d7-ba37-f45dae40db4a",
   "metadata": {
    "language": "sql",
    "name": "sql__ingestion_with_evolution__json__data"
   },
   "outputs": [],
   "source": [
    "select * from \"DATA_FROM_ALL_SOURCES\";"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f351dc8-f6d4-4b95-b6a3-e3ca71cd353f",
   "metadata": {
    "collapsed": false,
    "name": "docs__ingestion_with_evolution__parquet"
   },
   "source": [
    "## Ingestion with evolution - Parquet\n",
    "\n",
    "Finally, we ingest our example parquet file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d074a06-3971-4fbb-b133-92699ed47415",
   "metadata": {
    "language": "sql",
    "name": "sql__ingestion_with_evolution__parquet"
   },
   "outputs": [],
   "source": [
    "copy into \"DATA_FROM_ALL_SOURCES\"\n",
    "from '@\"STG__DATA\"/parquet'\n",
    "  file_format = \"FF_PARQUET\"\n",
    "  match_by_column_name = CASE_INSENSITIVE\n",
    "  include_metadata = (\n",
    "      \"METADATA_FILE_PATH\" = METADATA$FILENAME\n",
    "    , \"METADATA_FILE_ROW_NUMBER\" = METADATA$FILE_ROW_NUMBER\n",
    "    , \"METADATA_RECORD_INGESTION_TIMESTAMP\" = METADATA$START_SCAN_TIME\n",
    "  )\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94554edb-1be2-49e4-9fe7-1f9cb0a074c1",
   "metadata": {
    "collapsed": false,
    "name": "docs__ingestion_with_evolution__parquet__metadata"
   },
   "source": [
    "Again, we can now see the additional fields have been created in the target table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0603211d-f7d6-4b6d-995a-6a7af71447f9",
   "metadata": {
    "language": "sql",
    "name": "sql__ingestion_with_evolution__parquet__metadata"
   },
   "outputs": [],
   "source": [
    "desc table \"DATA_FROM_ALL_SOURCES\";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f5c1e2e-f5ca-41b2-b05d-933639baf3ec",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "py__ingestion_with_evolution__parquet__metadata"
   },
   "outputs": [],
   "source": [
    "st.markdown(\"\"\"Again, we can expand the \"schema evolution record\" in this output to see exactly where each field came from:\"\"\")\n",
    "\n",
    "display_schema_evolution_records(\n",
    "    snowflake_query_result = sql__ingestion_with_evolution__parquet__metadata\n",
    "  , source_type = \"parquet\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d5a339-5b53-44ad-80f7-0959f35df55d",
   "metadata": {
    "collapsed": false,
    "name": "docs__ingestion_with_evolution__parquet__data"
   },
   "source": [
    "Again, we can query the table to see how these new fields have been populated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629fe9f5-472e-45e9-823d-ebcb055664fa",
   "metadata": {
    "language": "sql",
    "name": "sql__ingestion_with_evolution__parquet__data"
   },
   "outputs": [],
   "source": [
    "select * from \"DATA_FROM_ALL_SOURCES\";"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
